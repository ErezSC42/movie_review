{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from TextCNN import TextCNN\n",
    "import gzip\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from Embedder import Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "FILENAME = \"movies.txt\"\n",
    "COLUMNS = [\"productId\",\n",
    "           \"userId\",\n",
    "           \"profileName\",\n",
    "           \"helpfulness\",\n",
    "           \"score\",\n",
    "           \"time\",\n",
    "           \"summary\",\n",
    "           \"text\"]\n",
    "COL_NUM = 8\n",
    "REVIEW_NUM = 140000\n",
    "PADDED_LEN = 200\n",
    "GLOVE_BINARY_PATH = \"glove.6B.50d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(FILENAME,\n",
    "                 header=None,\n",
    "                 delimiter=\"\\n\",\n",
    "                 error_bad_lines=False,\n",
    "                 skip_blank_lines=True,\n",
    "                 encoding=\"latin-1\",\n",
    "                 nrows=COL_NUM*REVIEW_NUM)\n",
    "df = pd.DataFrame(np.reshape(df.values,(REVIEW_NUM,COL_NUM)),columns=COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"productId\"] = df[\"productId\"].str.replace(\"product/productId:\",\"\")\n",
    "for col in COLUMNS[1:]:\n",
    "    df[col] = df[col].str.replace(\"review/\" + col + \":\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>productId</th>\n",
       "      <th>userId</th>\n",
       "      <th>profileName</th>\n",
       "      <th>helpfulness</th>\n",
       "      <th>score</th>\n",
       "      <th>time</th>\n",
       "      <th>summary</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B003AI2VGA</td>\n",
       "      <td>A141HP4LYPWMSR</td>\n",
       "      <td>Brian E. Erland \"Rainbow Sphinx\"</td>\n",
       "      <td>7/7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1182729600</td>\n",
       "      <td>\"There Is So Much Darkness Now ~ Come For The...</td>\n",
       "      <td>Synopsis: On the daily trek from Juarez, Mexi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B003AI2VGA</td>\n",
       "      <td>A328S9RN3U5M68</td>\n",
       "      <td>Grady Harp</td>\n",
       "      <td>4/4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1181952000</td>\n",
       "      <td>Worthwhile and Important Story Hampered by Po...</td>\n",
       "      <td>THE VIRGIN OF JUAREZ is based on true events ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B003AI2VGA</td>\n",
       "      <td>A1I7QGUDP043DG</td>\n",
       "      <td>Chrissy K. McVay \"Writer\"</td>\n",
       "      <td>8/10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1164844800</td>\n",
       "      <td>This movie needed to be made.</td>\n",
       "      <td>The scenes in this film can be very disquieti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B003AI2VGA</td>\n",
       "      <td>A1M5405JH9THP9</td>\n",
       "      <td>golgotha.gov</td>\n",
       "      <td>1/1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1197158400</td>\n",
       "      <td>distantly based on a real tragedy</td>\n",
       "      <td>THE VIRGIN OF JUAREZ (2006)&lt;br /&gt;directed by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B003AI2VGA</td>\n",
       "      <td>ATXL536YX71TR</td>\n",
       "      <td>KerrLines \"&amp;#34;Movies,Music,Theatre&amp;#34;\"</td>\n",
       "      <td>1/1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1188345600</td>\n",
       "      <td>\"What's going on down in Juarez and shining a...</td>\n",
       "      <td>Informationally, this SHOWTIME original is es...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     productId           userId                                  profileName  \\\n",
       "0   B003AI2VGA   A141HP4LYPWMSR             Brian E. Erland \"Rainbow Sphinx\"   \n",
       "1   B003AI2VGA   A328S9RN3U5M68                                   Grady Harp   \n",
       "2   B003AI2VGA   A1I7QGUDP043DG                    Chrissy K. McVay \"Writer\"   \n",
       "3   B003AI2VGA   A1M5405JH9THP9                                 golgotha.gov   \n",
       "4   B003AI2VGA    ATXL536YX71TR   KerrLines \"&#34;Movies,Music,Theatre&#34;\"   \n",
       "\n",
       "  helpfulness score         time  \\\n",
       "0         7/7   3.0   1182729600   \n",
       "1         4/4   3.0   1181952000   \n",
       "2        8/10   5.0   1164844800   \n",
       "3         1/1   3.0   1197158400   \n",
       "4         1/1   3.0   1188345600   \n",
       "\n",
       "                                             summary  \\\n",
       "0   \"There Is So Much Darkness Now ~ Come For The...   \n",
       "1   Worthwhile and Important Story Hampered by Po...   \n",
       "2                      This movie needed to be made.   \n",
       "3                  distantly based on a real tragedy   \n",
       "4   \"What's going on down in Juarez and shining a...   \n",
       "\n",
       "                                                text  \n",
       "0   Synopsis: On the daily trek from Juarez, Mexi...  \n",
       "1   THE VIRGIN OF JUAREZ is based on true events ...  \n",
       "2   The scenes in this film can be very disquieti...  \n",
       "3   THE VIRGIN OF JUAREZ (2006)<br />directed by ...  \n",
       "4   Informationally, this SHOWTIME original is es...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deep3/text/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X = df[\"text\"]\n",
    "y = df[\"score\"].astype(\"float\").astype(\"int\").values.reshape([-1,1])\n",
    "y_one_hot = OneHotEncoder().fit_transform(y).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df[\"score\"] == \"review/helpfulness: 0/0\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140000\n"
     ]
    }
   ],
   "source": [
    "embedder = Embedder(None,50,PADDED_LEN,GLOVE_BINARY_PATH)\n",
    "X_embedded = embedder.str_series_to_image(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.025\n",
    "FC_LAYER = 100\n",
    "CLASSES_LEN = 5\n",
    "EMBEDDING_DIM = 50\n",
    "CONV_FILTERS = 512\n",
    "EPOCHS = 100\n",
    "TRAIN_TEST_RATION = 0.15\n",
    "TRAIN_SIZE = (1 - TRAIN_TEST_RATION) * len(X_embedded)\n",
    "TEST_SIZE = (TRAIN_TEST_RATION) * len(X_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_embedded, y_one_hot, test_size=TRAIN_TEST_RATION)\n",
    "X_train = X_train.reshape([-1,1,PADDED_LEN,EMBEDDING_DIM])\n",
    "X_test = X_test.reshape([-1,1,PADDED_LEN,EMBEDDING_DIM])\n",
    "X_train_tensor = torch.Tensor(X_train).to(device)\n",
    "X_test_tensor = torch.Tensor(X_test).to(device)\n",
    "y_train_tensor = torch.Tensor(y_train).long().to(device)\n",
    "y_test_tensor = torch.Tensor(y_test).long().to(device)\n",
    "#y_train_tensor = torch.Tensor(y_train).to(device)\n",
    "#y_test_tensor = torch.Tensor(y_test).to(device)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor,y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor,y_test_tensor)\n",
    "trainloader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "testloader = DataLoader(test_dataset, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextCNN(\n",
       "  (conv3): Conv2d(1, 512, kernel_size=(3, 50), stride=(1, 1))\n",
       "  (conv4): Conv2d(1, 512, kernel_size=(4, 50), stride=(1, 1))\n",
       "  (conv5): Conv2d(1, 512, kernel_size=(5, 50), stride=(1, 1))\n",
       "  (Max3_pool): MaxPool2d(kernel_size=(198, 1), stride=(198, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "  (Max4_pool): MaxPool2d(kernel_size=(197, 1), stride=(197, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "  (Max5_pool): MaxPool2d(kernel_size=(196, 1), stride=(196, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=1536, out_features=100, bias=True)\n",
       "  (dropout1): Dropout(p=0.5)\n",
       "  (linear1): Linear(in_features=100, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TextCNN(hidden_units=FC_LAYER,\n",
    "                output_len=CLASSES_LEN,\n",
    "                textcnn_filter_count=CONV_FILTERS,\n",
    "                sentence_max_size=PADDED_LEN,\n",
    "                word_embedding_dimension=EMBEDDING_DIM)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),weight_decay=1e-5)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119000, 1, 200, 50)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy_classification(data_loader,model,name):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total = len(data_loader)\n",
    "    testloader.dataset.tensors[0]\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            labels = torch.max(labels, 1)[1]\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy of the network on the '+name +' reviews: %d %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy_mse(data_loader,model,name):\n",
    "    batch_losses = []\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            batch_losses.append(loss.item())\n",
    "    #print(batch_losses)\n",
    "    mse = np.array(batch_losses).mean()\n",
    "    print('MSE of the network on the '+name +' reviews: %f' % mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   117] loss: 50.317\n",
      "Accuracy of the network on the train reviews: 63 %\n",
      "Accuracy of the network on the test reviews: 62 %\n",
      "[2,   117] loss: 41.768\n",
      "Accuracy of the network on the train reviews: 73 %\n",
      "Accuracy of the network on the test reviews: 72 %\n",
      "[3,   117] loss: 38.650\n",
      "Accuracy of the network on the train reviews: 74 %\n",
      "Accuracy of the network on the test reviews: 73 %\n",
      "[4,   117] loss: 36.546\n",
      "Accuracy of the network on the train reviews: 74 %\n",
      "Accuracy of the network on the test reviews: 71 %\n",
      "[5,   117] loss: 35.326\n",
      "Accuracy of the network on the train reviews: 76 %\n",
      "Accuracy of the network on the test reviews: 72 %\n",
      "[6,   117] loss: 34.415\n",
      "Accuracy of the network on the train reviews: 77 %\n",
      "Accuracy of the network on the test reviews: 73 %\n",
      "[7,   117] loss: 33.578\n",
      "Accuracy of the network on the train reviews: 77 %\n",
      "Accuracy of the network on the test reviews: 73 %\n",
      "[8,   117] loss: 32.747\n",
      "Accuracy of the network on the train reviews: 78 %\n",
      "Accuracy of the network on the test reviews: 73 %\n",
      "[9,   117] loss: 32.468\n",
      "Accuracy of the network on the train reviews: 78 %\n",
      "Accuracy of the network on the test reviews: 73 %\n",
      "[10,   117] loss: 31.986\n",
      "Accuracy of the network on the train reviews: 78 %\n",
      "Accuracy of the network on the test reviews: 72 %\n",
      "[11,   117] loss: 31.750\n",
      "Accuracy of the network on the train reviews: 78 %\n",
      "Accuracy of the network on the test reviews: 73 %\n",
      "[12,   117] loss: 31.383\n",
      "Accuracy of the network on the train reviews: 78 %\n",
      "Accuracy of the network on the test reviews: 72 %\n",
      "[13,   117] loss: 31.541\n",
      "Accuracy of the network on the train reviews: 78 %\n",
      "Accuracy of the network on the test reviews: 72 %\n",
      "[14,   117] loss: 30.763\n",
      "Accuracy of the network on the train reviews: 76 %\n",
      "Accuracy of the network on the test reviews: 69 %\n",
      "[15,   117] loss: 30.569\n",
      "Accuracy of the network on the train reviews: 79 %\n",
      "Accuracy of the network on the test reviews: 72 %\n",
      "[16,   117] loss: 30.238\n",
      "Accuracy of the network on the train reviews: 79 %\n",
      "Accuracy of the network on the test reviews: 72 %\n",
      "[17,   117] loss: 30.201\n",
      "Accuracy of the network on the train reviews: 79 %\n",
      "Accuracy of the network on the test reviews: 72 %\n",
      "[18,   117] loss: 30.492\n",
      "Accuracy of the network on the train reviews: 78 %\n",
      "Accuracy of the network on the test reviews: 71 %\n",
      "[19,   117] loss: 30.540\n",
      "Accuracy of the network on the train reviews: 79 %\n",
      "Accuracy of the network on the test reviews: 71 %\n",
      "[20,   117] loss: 30.010\n",
      "Accuracy of the network on the train reviews: 79 %\n",
      "Accuracy of the network on the test reviews: 71 %\n",
      "[21,   117] loss: 29.785\n",
      "Accuracy of the network on the train reviews: 78 %\n",
      "Accuracy of the network on the test reviews: 71 %\n",
      "[22,   117] loss: 30.580\n",
      "Accuracy of the network on the train reviews: 73 %\n",
      "Accuracy of the network on the test reviews: 66 %\n",
      "[23,   117] loss: 32.045\n",
      "Accuracy of the network on the train reviews: 79 %\n",
      "Accuracy of the network on the test reviews: 71 %\n",
      "[24,   117] loss: 29.726\n",
      "Accuracy of the network on the train reviews: 79 %\n",
      "Accuracy of the network on the test reviews: 70 %\n",
      "[25,   117] loss: 29.304\n",
      "Accuracy of the network on the train reviews: 79 %\n",
      "Accuracy of the network on the test reviews: 71 %\n",
      "[26,   117] loss: 29.920\n",
      "Accuracy of the network on the train reviews: 80 %\n",
      "Accuracy of the network on the test reviews: 71 %\n",
      "[27,   117] loss: 29.071\n",
      "Accuracy of the network on the train reviews: 76 %\n",
      "Accuracy of the network on the test reviews: 68 %\n",
      "[28,   117] loss: 30.828\n",
      "Accuracy of the network on the train reviews: 80 %\n",
      "Accuracy of the network on the test reviews: 71 %\n",
      "[29,   117] loss: 29.421\n",
      "Accuracy of the network on the train reviews: 80 %\n",
      "Accuracy of the network on the test reviews: 71 %\n",
      "[30,   117] loss: 29.094\n",
      "Accuracy of the network on the train reviews: 79 %\n",
      "Accuracy of the network on the test reviews: 70 %\n",
      "[31,   117] loss: 28.979\n",
      "Accuracy of the network on the train reviews: 78 %\n",
      "Accuracy of the network on the test reviews: 68 %\n",
      "[32,   117] loss: 30.253\n",
      "Accuracy of the network on the train reviews: 79 %\n",
      "Accuracy of the network on the test reviews: 69 %\n",
      "[33,   117] loss: 29.399\n",
      "Accuracy of the network on the train reviews: 80 %\n",
      "Accuracy of the network on the test reviews: 70 %\n",
      "[34,   117] loss: 31.473\n",
      "Accuracy of the network on the train reviews: 78 %\n",
      "Accuracy of the network on the test reviews: 68 %\n",
      "[35,   117] loss: 29.905\n",
      "Accuracy of the network on the train reviews: 80 %\n",
      "Accuracy of the network on the test reviews: 69 %\n",
      "[36,   117] loss: 29.126\n",
      "Accuracy of the network on the train reviews: 80 %\n",
      "Accuracy of the network on the test reviews: 70 %\n",
      "[37,   117] loss: 28.021\n",
      "Accuracy of the network on the train reviews: 80 %\n",
      "Accuracy of the network on the test reviews: 69 %\n",
      "[38,   117] loss: 28.262\n",
      "Accuracy of the network on the train reviews: 81 %\n",
      "Accuracy of the network on the test reviews: 70 %\n",
      "[39,   117] loss: 28.217\n",
      "Accuracy of the network on the train reviews: 81 %\n",
      "Accuracy of the network on the test reviews: 70 %\n",
      "[40,   117] loss: 28.543\n",
      "Accuracy of the network on the train reviews: 78 %\n",
      "Accuracy of the network on the test reviews: 67 %\n",
      "[41,   117] loss: 28.597\n",
      "Accuracy of the network on the train reviews: 81 %\n",
      "Accuracy of the network on the test reviews: 70 %\n",
      "[42,   117] loss: 28.247\n",
      "Accuracy of the network on the train reviews: 78 %\n",
      "Accuracy of the network on the test reviews: 67 %\n",
      "[43,   117] loss: 32.497\n",
      "Accuracy of the network on the train reviews: 77 %\n",
      "Accuracy of the network on the test reviews: 69 %\n",
      "[44,   117] loss: 29.161\n",
      "Accuracy of the network on the train reviews: 77 %\n",
      "Accuracy of the network on the test reviews: 66 %\n",
      "[45,   117] loss: 28.518\n",
      "Accuracy of the network on the train reviews: 80 %\n",
      "Accuracy of the network on the test reviews: 69 %\n",
      "[46,   117] loss: 28.865\n",
      "Accuracy of the network on the train reviews: 78 %\n",
      "Accuracy of the network on the test reviews: 69 %\n",
      "[47,   117] loss: 28.362\n",
      "Accuracy of the network on the train reviews: 80 %\n",
      "Accuracy of the network on the test reviews: 69 %\n",
      "[48,   117] loss: 29.506\n",
      "Accuracy of the network on the train reviews: 80 %\n",
      "Accuracy of the network on the test reviews: 69 %\n",
      "[49,   117] loss: 30.232\n",
      "Accuracy of the network on the train reviews: 80 %\n",
      "Accuracy of the network on the test reviews: 68 %\n",
      "[50,   117] loss: 27.743\n",
      "Accuracy of the network on the train reviews: 80 %\n",
      "Accuracy of the network on the test reviews: 68 %\n",
      "[51,   117] loss: 28.981\n",
      "Accuracy of the network on the train reviews: 78 %\n",
      "Accuracy of the network on the test reviews: 68 %\n",
      "[52,   117] loss: 27.675\n",
      "Accuracy of the network on the train reviews: 81 %\n",
      "Accuracy of the network on the test reviews: 69 %\n",
      "[53,   117] loss: 27.622\n",
      "Accuracy of the network on the train reviews: 80 %\n",
      "Accuracy of the network on the test reviews: 69 %\n",
      "[54,   117] loss: 27.502\n",
      "Accuracy of the network on the train reviews: 81 %\n",
      "Accuracy of the network on the test reviews: 69 %\n",
      "[55,   117] loss: 30.293\n",
      "Accuracy of the network on the train reviews: 70 %\n",
      "Accuracy of the network on the test reviews: 61 %\n",
      "[56,   117] loss: 29.682\n",
      "Accuracy of the network on the train reviews: 79 %\n",
      "Accuracy of the network on the test reviews: 68 %\n",
      "[57,   117] loss: 27.474\n",
      "Accuracy of the network on the train reviews: 80 %\n",
      "Accuracy of the network on the test reviews: 68 %\n",
      "[58,   117] loss: 27.931\n",
      "Accuracy of the network on the train reviews: 80 %\n",
      "Accuracy of the network on the test reviews: 68 %\n",
      "[59,   117] loss: 27.816\n",
      "Accuracy of the network on the train reviews: 81 %\n",
      "Accuracy of the network on the test reviews: 68 %\n",
      "[60,   117] loss: 28.032\n",
      "Accuracy of the network on the train reviews: 79 %\n",
      "Accuracy of the network on the test reviews: 67 %\n",
      "[61,   117] loss: 28.363\n",
      "Accuracy of the network on the train reviews: 80 %\n",
      "Accuracy of the network on the test reviews: 68 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-c4264024106b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# loop over the dataset multiple times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;31m# get the inputs; data is a list of [inputs, labels]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/text/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/text/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        #loss = criterion(outputs,labels)\n",
    "        loss = criterion(outputs, torch.max(labels, 1)[1])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        #if i % 100 == 0:    # print every 2000 mini-batches\n",
    "    print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2))\n",
    "    #print('epoch: [%d] loss: %.3f' % (epoch + 1, running_loss))\n",
    "    check_accuracy_classification(trainloader,model,\"train\")\n",
    "    check_accuracy_classification(testloader,model,\"test\")\n",
    "#     check_accuracy_mse(trainloader,model,\"train\")\n",
    "#     check_accuracy_mse(testloader,model,\"test\")\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        labels = torch.max(labels, 1)[1]\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the network on the '+ str(TEST_SIZE) +' test reviews: %d %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in trainloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        labels = torch.max(labels, 1)[1]\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the network on the ' + str(TRAIN_SIZE) + ' train reviews: %d %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review: the movie was okay at best. rank: 3\n",
      "review: the movie was trash. rank: 1\n",
      "review: this is the greatest thing I have ever seen!. rank: 5\n"
     ]
    }
   ],
   "source": [
    "reviews = [\"the movie was okay at best\",\"the movie was trash\",\"this is the greatest thing I have ever seen!\"]\n",
    "with torch.no_grad():\n",
    "    for review in reviews:\n",
    "        review_data = torch.Tensor(embedder.str_to_image(review).reshape([-1,1,PADDED_LEN,EMBEDDING_DIM])).long().to(device)\n",
    "        res = model(review_data)\n",
    "        print(\"review: \" + review + \". rank: \" + str(torch.argmax(res).item() + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 4, 1,  ..., 4, 1, 3])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(labels, 1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text",
   "language": "python",
   "name": "text"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
